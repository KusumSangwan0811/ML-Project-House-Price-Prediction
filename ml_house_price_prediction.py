# -*- coding: utf-8 -*-
"""ML_House_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UU8Zj2ZTG9FpKsm_3IoZ3i4gyl7p7mxb

# House Price Prediction with ML Models (Kaggle Competition)

![](https://i.imgur.com/3sw1fY9.jpg)

In this assignment, we predict the price of a house using information like its location, area, no. of rooms etc.


Dataset is available at [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition on [Kaggle](https://kaggle.com).


**Step-by-step process:**
1. Download and explore the data
2. Prepare the dataset for training
3. Train a linear regression model, make predictions and evaluate the model
4. Train, evaluate and interpret decision tree and random forest
5. Tune hyperparameters to improve the model
6. Make predictions and save the model




 This is a project, I have done in the course [Machine Learning with Python: Zero to GBMs](https://jovian.ai/learn/machine-learning-with-python-zero-to-gbms).

Let's begin by installing the required libraries:
"""

import numpy
import pandas as pd
import matplotlib
import seaborn
import plotly
#opendatasets

"""## Step 1 - Download and Explore the Data

The dataset is available as a ZIP file at the following url:
"""

dataset_url = 'https://github.com/JovianML/opendatasets/raw/master/data/house-prices-advanced-regression-techniques.zip'

"""We'll use the `urlretrieve` function from the module [`urllib.request`](https://docs.python.org/3/library/urllib.request.html) to dowload the dataset."""

from urllib.request import urlretrieve

urlretrieve(dataset_url, 'house-prices.zip')

"""The file `housing-prices.zip` has been downloaded. Let's unzip it using the [`zipfile`](https://docs.python.org/3/library/zipfile.html) module."""

from zipfile import ZipFile

with ZipFile('house-prices.zip') as f:
    f.extractall(path='house-prices')

"""The dataset is extracted to the folder `house-prices`. Let's view the contents of the folder using the [`os`](https://docs.python.org/3/library/os.html) module."""

import os

data_dir = 'house-prices'

os.listdir(data_dir)

"""Use the "File" > "Open" menu option to browse the contents of each file. You can also check out the [dataset description](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) on Kaggle to learn more.

We'll use the data in the file `train.csv` for training our model. We can load the for processing using the [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html) library.
"""

import pandas as pd
pd.options.display.max_columns = 200
pd.options.display.max_rows = 200

train_csv_path = data_dir + '/train.csv'
train_csv_path

"""Loading the data from the file `train.csv` into a Pandas data frame."""

prices_df = pd.read_csv('house-prices/train.csv')

prices_df

"""Let's explore the columns and data types within the dataset."""

prices_df.info()

"""How many rows and columns does the dataset contain?"""

n_rows = prices_df.shape[0]
n_rows

n_cols = prices_df.shape[1]
n_cols

print('The dataset contains {} rows and {} columns.'.format(n_rows, n_cols))

""">Before training the model, we explore and visualize data from the various columns within the dataset, and study their relationship with the price of the house (using scatter plot and correlations). Create some graphs and summarize your insights using the empty cells below."""

# Commented out IPython magic to ensure Python compatibility.
!pip install plotly matplotlib seaborn --quiet
import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10, 6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

prices_df.SalePrice.describe()

fig = px.scatter(prices_df,
                 x='SalePrice',
                 y='LotArea',
                 color='LotShape',
                 opacity=0.8,
                 title='SalePrice vs LotArea')
fig.update_traces(marker_size=5)
fig.show()

prices_df.HouseStyle.describe()

fig = px.scatter(prices_df,
                 x='SalePrice',
                 y='YearBuilt',
                 color='LotShape',
                 opacity=0.8,

                 title='SalePrice vs YearBuilt')
fig.update_traces(marker_size=5)
fig.show()



"""## Step 2 - Prepare the Dataset for Training

Before we can train the model, we need to prepare the dataset. Here are the steps we'll follow:

1. Identify the input and target column(s) for training the model.
2. Identify numeric and categorical input columns.
3. [Impute](https://scikit-learn.org/stable/modules/impute.html) (fill) missing values in numeric columns
4. [Scale](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a $(0,1)$ range.
5. [Encode](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) categorical data into one-hot vectors.
6. Split the dataset into training and validation sets.

### Identify Inputs and Targets

While the dataset contains 81 columns, not all of them are useful for modeling. Note the following:

- The first column `Id` is a unique ID for each house and isn't useful for training the model.
- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.
- Data from all the other columns (except the first and the last column) can be used as inputs to the model.
"""

prices_df

""">Create a list `input_cols` of column names containing data that can be used as input to train the model, and identify the target column as the variable `target_col`."""

# Identify the input columns (a list of column names)
input_cols = ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',
       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',
       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',
       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',
       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',
       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',
       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',
       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',
       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',
       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',
       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',
       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',
       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',
       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',
       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',
       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',
       'SaleCondition']

# Identify the name of the target column (a single string, not a list)
target_col = 'SalePrice'

print(list(input_cols))

len(input_cols)

print(target_col)

"""Make sure that the `Id` and `SalePrice` columns are not included in `input_cols`.

Now that we've identified the input and target columns, we can separate input & target data.
"""

inputs_df = prices_df[input_cols].copy()

targets = prices_df[target_col]

inputs_df

targets

"""Let's save our work before continuing.

### Identify Numeric and Categorical Data

The next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column.
"""

prices_df.info()



"""> Crate two lists `numeric_cols` and `categorical_cols` containing names of numeric and categorical input columns within the dataframe respectively. Numeric columns have data types `int64` and `float64`, whereas categorical columns have the data type `object`.

"""

import numpy as np

numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()

categorical_cols = inputs_df.select_dtypes(include=['object']).columns.tolist()

print(list(numeric_cols))

print(list(categorical_cols))

"""Let's save our work before continuing.

### Impute Numerical Data

Some of the numeric columns in our dataset contain missing values (`nan`).
"""

missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)
missing_counts[missing_counts > 0]

"""Machine learning models can't work with missing data. The process of filling missing values is called [imputation](https://scikit-learn.org/stable/modules/impute.html).

<img src="https://i.imgur.com/W7cfyOp.png" width="480">

There are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`.

"""

from sklearn.impute import SimpleImputer

"""> Impute (fill) missing values in the numeric columns of `inputs_df` using a `SimpleImputer`.

"""

# 1. Create the imputer
imputer = SimpleImputer(strategy='mean')

# 2. Fit the imputer to the numeric colums
imputer.fit(prices_df[numeric_cols])

# 3. Transform and replace the numeric columns
inputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])

"""After imputation, none of the numeric columns should contain any missing values."""

missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)
missing_counts[missing_counts > 0] # should be an empty list

"""Let's save our work before continuing.

### Scale Numerical Values

The numeric columns in our dataset have varying ranges.
"""

inputs_df[numeric_cols].describe().loc[['min', 'max']]

"""A good practice is to [scale numeric features](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) to a small range of values e.g. $(0,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.

> Scale numeric values to the $(0, 1)$ range using `MinMaxScaler` from `sklearn.preprocessing`.
"""

from sklearn.preprocessing import MinMaxScaler

# Create the scaler
scaler = MinMaxScaler()

# Fit the scaler to the numeric columns
scaler.fit(prices_df[numeric_cols])

# Transform and replace the numeric columns
inputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])

"""After scaling, the ranges of all numeric columns should be $(0, 1)$."""

inputs_df[numeric_cols].describe().loc[['min', 'max']]

"""Let's save our work before continuing.

### Encode Categorical Columns

Our dataset contains several categorical columns, each with a different number of categories.
"""

inputs_df[categorical_cols].nunique().sort_values(ascending=False)

"""Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.

<img src="https://i.imgur.com/n8GuiOO.png" width="640">

One hot encoding involves adding a new binary (0/1) column for each unique category of a categorical column.

> Encode categorical columns in the dataset as one-hot vectors using `OneHotEncoder` from `sklearn.preprocessing`. Add a new binary (0/1) column for each category
"""

from sklearn.preprocessing import OneHotEncoder

# 1. Create the encoder
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')

# 2. Fit the encoder to the categorical colums
encoder.fit(prices_df[categorical_cols])

# 3. Generate column names for each category
encoded_cols = list(encoder.get_feature_names_out(categorical_cols))
len(encoded_cols)

# 4. Transform and add new one-hot category columns
inputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])

"""The new one-hot category columns should now be added to `inputs_df`."""

inputs_df

"""### Training and Validation Set

Finally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers.
"""

from sklearn.model_selection import train_test_split

train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df[numeric_cols + encoded_cols],
                                                                        targets,
                                                                        test_size=0.25,
                                                                        random_state=42)

train_inputs

train_targets

val_inputs

val_targets

"""## Step 3 - Ridge Regression Model

In a linear regression model, the target is modeled as a linear combination (or weighted sum) of input features. The predictions from the model are evaluated using a loss function like the Root Mean Squared Error (RMSE).

The structure of a linear regression model is shown below:

<img src="https://i.imgur.com/iTM2s5k.png" width="480">

However, linear regression doesn't generalize very well when we have a large number of input columns with co-linearity, that is, when the values one column are highly correlated with values in other column(s). This is because it tries to fit the training data perfectly.

Instead, we'll use Ridge Regression, a variant of linear regression that uses a technique called "L2 regularization" to introduce another loss term that forces the model to generalize better. Learn more about ridge regression here: https://www.youtube.com/watch?v=Q81RR3yKn30
"""

from sklearn.linear_model import Ridge

# Create the model
model = Ridge()

# Fit the model using inputs and targets
model.fit(train_inputs,train_targets)

"""## Make Predictions and Evaluate Your Model

The model is now trained, and we can use it to generate predictions for the training and validation inputs. We can evaluate the model's performance using the RMSE (root mean squared error) loss function.

> Generate predictions and compute the RMSE loss for the training and validation sets.
"""

from sklearn.metrics import mean_squared_error

train_preds =  model.predict(train_inputs)

train_preds

train_rmse = mean_squared_error(train_targets,train_preds,squared=False)

print('The RMSE loss for the training set is $ {}.'.format(train_rmse))

val_preds =  model.predict(val_inputs)

val_preds

val_rmse = mean_squared_error(val_targets,val_preds,squared=False)

print('The RMSE loss for the validation set is $ {}.'.format(val_rmse))

"""### Feature Importance

Let's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important.

> Identify the weights (or coefficients) assigned to for different features by the model.
"""

weights = model.coef_
len(weights)

"""Let's create a dataframe to view the weight assigned to each column."""

weights_df = pd.DataFrame({
    'columns': train_inputs.columns,
    'weight': weights
}).sort_values('weight', ascending=False)

weights_df



"""# 2. Decision Tree

Train a Decision Tree Model.
"""

from sklearn.tree import DecisionTreeRegressor

# Create the model
tree = DecisionTreeRegressor(random_state=42)

# Fit the model to the training data
tree.fit(train_inputs, train_targets)

""">Generate predictions on the training and validation sets using the trained decision tree, and compute the RMSE loss."""

from sklearn.metrics import mean_squared_error

tree_train_preds = tree.predict(train_inputs)
tree_train_rmse = mean_squared_error(train_targets,tree_train_preds,squared=False)
tree_val_preds = tree.predict(val_inputs)
tree_val_rmse = mean_squared_error(val_targets,tree_val_preds,squared=False)

print('Train RMSE: {}, Validation RMSE: {}'.format(tree_train_rmse, tree_val_rmse))



""">Visualize the decision tree (graphically and textually) and display feature importances as a graph. Limit the maximum depth of graphical visualization to 4 levels."""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree, export_text
import seaborn as sns
sns.set_style('darkgrid')
# %matplotlib inline

plt.figure(figsize=(30,15))

# Visualize the tree graphically using plot_tree
x=plot_tree(tree, feature_names= train_inputs.columns, max_depth=3, filled=True);

# Visualize the tree textually using export_text
tree_text = export_text(tree)

# Display the first few lines
print(tree_text[:2000])

# Check feature importance
tree_importances =  tree.tree_.compute_feature_importances(tree)

tree_importance_df = pd.DataFrame({
    'feature': train_inputs.columns,
    'importance': tree_importances
}).sort_values('importance', ascending=False)

tree_importance_df

plt.title('Decision Tree Feature Importance')
sns.barplot(data=tree_importance_df.head(10), x='importance', y='feature')

"""# Random Forest

Train Random Forest regressor using the training data.
"""

from sklearn.ensemble import RandomForestRegressor

# Create the model
rf1 = RandomForestRegressor(random_state=42)

# Fit the model
rf1.fit(train_inputs,train_targets)

rf1_train_preds = rf1.predict(train_inputs)
rf1_train_rmse = mean_squared_error(train_targets,rf1_train_preds,squared=False)
rf1_val_preds = rf1.predict(val_inputs)
rf1_val_rmse = mean_squared_error(val_targets,rf1_val_preds,squared=False)
print('Train RMSE: {}, Validation RMSE: {}'.format(rf1_train_rmse, rf1_val_rmse))

"""## Hyperparameter Tuning

Let us now tune the hyperparameters of our model. Find the hyperparameters for `RandomForestRegressor` here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html

<img src="https://i.imgur.com/EJCrSZw.png" width="480">


Let's define a helper function `test_params` which can test the given value of one or more hyperparameters.
"""

def test_params(**params):
    model = RandomForestRegressor(random_state=42, n_jobs=-1, **params).fit(train_inputs, train_targets)
    train_rmse = mean_squared_error(model.predict(train_inputs), train_targets, squared=False)
    val_rmse = mean_squared_error(model.predict(val_inputs), val_targets, squared=False)
    return train_rmse, val_rmse

test_params(n_estimators=20, max_depth=20)

test_params(n_estimators=50, max_depth=10, min_samples_leaf=4, max_features=0.4)

""">Let's also define a helper function to test and plot different values of a single parameter."""

def test_param_and_plot(param_name, param_values):
    train_errors, val_errors = [], []
    for value in param_values:
        params = {param_name: value}
        train_rmse, val_rmse = test_params(**params)
        train_errors.append(train_rmse)
        val_errors.append(val_rmse)
    plt.figure(figsize=(10,6))
    plt.title('Overfitting curve: ' + param_name)
    plt.plot(param_values, train_errors, 'b-o')
    plt.plot(param_values, val_errors, 'r-o')
    plt.xlabel(param_name)
    plt.ylabel('RMSE')
    plt.legend(['Training', 'Validation'])

test_param_and_plot('max_depth', [5, 10, 15, 20, 25, 30, 35])

"""From the above graph, it appears that the best value for `max_depth` is around 20, beyond which the model starts to overfit.

We use the `test_params` and `test_param_and_plot` functions to experiment with different values of the  hyperparmeters like `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_features`, `max_leaf_nodes`, `min_impurity_decrease`, `min_impurity_split` etc. Learn more about the hyperparameters here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html
"""

test_param_and_plot('max_features' , [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85])

test_param_and_plot('n_estimators', [1, 5, 10, 15, 20, 25, 30, 35,40,45,50])

test_params(max_features='log2')

test_params(max_features=3)

test_params(max_leaf_nodes=2**5)

test_params(max_leaf_nodes=2**10)

test_params(min_samples_split=2, min_samples_leaf=2)

test_params(min_samples_split=6, min_samples_leaf=2)

test_param_and_plot('min_samples_split', [2, 3, 4, 5, 6, 7,8,9,10,12])

test_param_and_plot('min_samples_leaf', [1, 2, 3, 4, 5, 6, 7,8,9,10])



"""## Training the Best Model

> We train a random forest regressor model with our best hyperparameters to minimize the validation loss.
"""

# Create the model with custom hyperparameters
#rf2 = RandomForestRegressor(n_estimators=50,max_leaf_nodes=2**20,max_features='log2',random_state=42)
#rf2 = RandomForestRegressor(n_estimators=50,max_depth=20,max_leaf_nodes=2**10,random_state=42)
#rf2 = RandomForestRegressor(n_estimators=50,min_samples_split=2, min_samples_leaf=2,random_state=42)
rf2=RandomForestRegressor(random_state=42)

# Train the model
rf2.fit(train_inputs, train_targets)

""">Make predictions and evaluate your final model. If you're unhappy with the results, modify the hyperparameters above and try again."""

rf2_train_preds =rf2.predict(train_inputs)
rf2_train_rmse = mean_squared_error(train_targets,rf2_train_preds,squared=False)
rf2_val_preds = rf2.predict(val_inputs)
rf2_val_rmse = mean_squared_error(val_targets,rf2_val_preds,squared=False)

print('Train RMSE: {}, Validation RMSE: {}'.format(rf2_train_rmse, rf2_val_rmse))

"""Let's also view and plot the feature importances."""

rf2_importance_df = pd.DataFrame({
    'feature': train_inputs.columns,
    'importance': rf2.feature_importances_
}).sort_values('importance', ascending=False)

rf2_importance_df

from matplotlib.pyplot import figure
figure(figsize=(8, 8))
sns.barplot(data=rf2_importance_df.head(10), x='importance', y='feature')



"""# Making Predictions on the Test Set

Let's make predictions on the test set provided with the data.
"""

test_df = pd.read_csv('house-prices/test.csv')

test_df



"""First, we need to reapply all the preprocessing steps."""

# Apply the imputer and scaler transformations to the numeric columns
test_df[numeric_cols] = scaler.transform(imputer.transform(test_df[numeric_cols]))

# Apply the encoder transformation to the categorical columns
test_df[encoded_cols] = encoder.transform(test_df[categorical_cols])

test_inputs = test_df[numeric_cols + encoded_cols]

"""We can now make predictions using our final model."""

test_preds = rf2.predict(test_inputs)

submission_df = pd.read_csv('house-prices/sample_submission.csv')

submission_df

"""Let's replace the values of the `SalePrice` column with our predictions."""

submission_df['SalePrice'] = test_preds

"""Let's save it as a CSV file and download it."""



submission_df.to_csv('submission.csv', index=False)

from IPython.display import FileLink
FileLink('submission.csv') # Doesn't work on Colab, use the file browser instead to download the file.

"""Submit this file to the competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submissions

![](https://i.imgur.com/6h2vXRq.png)
"""



"""### Saving the model

Let's save the model (along with other useful objects) to disk, so that we use it for making predictions without retraining.
"""

import joblib

house_price_predictor = {
    'model': model,
    'imputer': imputer,
    'scaler': scaler,
    'encoder': encoder,
    'input_cols': input_cols,
    'target_col': target_col,
    'numeric_cols': numeric_cols,
    'categorical_cols': categorical_cols,
    'encoded_cols': encoded_cols
}

joblib.dump(house_price_predictor, 'house_price_predictor.joblib')

